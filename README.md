# ğŸš€ ClusterForge â€” Multi-Environment GitOps Platform for Kubernetes:

A distributed multi-cluster Kubernetes system designed to manage multiple environments, where infrastructure provisioning, application deployment, scaling, and monitoring are fully automated using GitOps workflows, with built-in scalability and observability powered by Terraform, AWS EKS, and ArgoCD.

***This project demonstrates how real Dev, Prod, and Control environments can be managed declaratively and reproducibly.***

# ğŸ§© Problem vs Solution (Real-World Production Context) :

| ğŸš¨ Real-World Problem | âŒ What Typically Happens in Teams | âœ… ClusterForge Solution |
|-----------------------|-----------------------------------|--------------------------|
| ğŸŒ Dev works, Prod breaks | Dev works, Prod breaks  Manual configuration differences between clusters. | Terraform modules create identical, reproducible clusters. |
| ğŸ” â€œWho changed this?â€ incidents | ```kubectl apply``` manually Cluster state diverges from Git. | ArgoCD enforces Git as the single source of truth. |
| â³ Traffic drops during deployment | Pods are terminated before new ones are ready; users see downtime | Rolling update strategy with readiness & liveness probes |
| ğŸ“‰ Application crashes during traffic spike | Static replica count; no autoscaling; manual intervention required | HPA dynamically adjusts replicas based on CPU metrics |
| ğŸ” Incident debugging takes hours | Teams check only `kubectl logs`; no metrics visibility | Prometheus monitoring stack provides real-time metrics and observability |
| ğŸ—ï¸ No one knows how infra was created | Click-ops in AWS console; no documentation; hard to recreate environments | Fully declarative Infrastructure as Code (Terraform) |
| ğŸ” Over-permissioned IAM roles | Static credentials and broad policies increase security risk | IAM roles with least privilege + OIDC provider integration |
| ğŸ’¥ Terraform destroy fails midway | AWS resources have hidden dependencies (e.g., NAT â†’ subnets â†’ VPC); incorrect deletion order causes failures and manual cleanup | Dependencies are explicitly handled and validated, ensuring clean and complete teardown of infrastructure |
| ğŸŒ Flat networking causes exposure | All services share same subnet; poor isolation between workloads | Multi-AZ VPC with public/private subnet isolation |
| ğŸ“¦ Dev accidentally affects Prod | Single cluster used for multiple environments | Dedicated EKS clusters per environment |
| ğŸ“Š Monitoring added after outage | Metrics and alerting introduced only after a production incident | Monitoring integrated as a core platform layer |
| ğŸ”„ Cluster management chaos | Multiple clusters manually accessed and configured | Central control cluster managing environments via GitOps |


## ğŸ“‚ Project Structure:

The following represents the folder structure of the **ClusterForge Infrastructure Repository**, responsible for provisioning networking, security, and multi-environment Kubernetes clusters using Terraform.

```
clusterforge-infra/
â”‚
â”œâ”€â”€ modules/  # Reusable Terraform modules
â”‚
â”‚   â”œâ”€â”€ vpc/  # VPC, subnets, routing, NAT, gateways
â”‚   â”‚   â”œâ”€â”€ main.tf        # Defines networking resources
â”‚   â”‚   â”œâ”€â”€ variables.tf   # CIDR, AZs, subnet configs
â”‚   â”‚   â””â”€â”€ outputs.tf     # VPC ID, subnet IDs
â”‚
â”‚   â”œâ”€â”€ eks/  # EKS cluster + node groups
â”‚   â”‚   â”œâ”€â”€ main.tf        # EKS, node groups, IRSA
â”‚   â”‚   â”œâ”€â”€ variables.tf   # Cluster config inputs
â”‚   â”‚   â””â”€â”€ outputs.tf     # Endpoint, OIDC, node details
â”‚
â”‚   â””â”€â”€ iam/  # IAM roles and policies
â”‚       â”œâ”€â”€ main.tf        # Roles, policies, OIDC trust
â”‚       â”œâ”€â”€ variables.tf   # Role configs
â”‚       â””â”€â”€ outputs.tf     # Role ARNs
â”‚
â”œâ”€â”€ main.tf         # Root module wiring VPC, IAM, EKS
â”œâ”€â”€ variables.tf    # Global configuration variables
â”œâ”€â”€ outputs.tf      # Exported infrastructure outputs
â”œâ”€â”€ providers.tf    # AWS provider configuration
â”œâ”€â”€ backend.tf      # Remote state (S3 + DynamoDB)
â”œâ”€â”€ README.md       # Project documentation
â”œâ”€â”€ LICENSE         # License file
â””â”€â”€ .gitignore      # Ignore local/terraform files

```
### ğŸ”— How Modules Work Together

- VPC â†’ provides networking  
- IAM â†’ provides permissions  
- EKS â†’ uses both to create clusters  

ğŸ‘‰ Modules are separate but connected through inputs/outputs.

The application deployment layer of this project â€” including Kubernetes manifests, ArgoCD configuration, and the multi-environment GitOps workflow â€” is maintained in a separate repository.

ğŸ”— **ClusterForge GitOps Repository:**  
https://github.com/immanas/clusterforge-gitops

### ğŸ“‚ Repository Separation (Why two repos?)
- clusterforge-infra â†’ builds infrastructure  
- clusterforge-gitops â†’ deploys applications  

ğŸ‘‰ Infra creates the platform, GitOps manages what runs on it.

##  ğŸ—ï¸ Architecture Diagram:

![ArgoCD Sync](cla.png)

## ğŸ“ˆ Core Features:

| âœ… What This Project **IS** | âŒ What This Project is **NOT** |
|--------------------------|------------------------------|
| Multi-Environment Kubernetes Platform â€” Dev, Prod, and Control clusters running on Amazon EKS | Not a single-cluster Kubernetes demo |
| Infrastructure as Code (Terraform) â€” Fully provisioned VPC, IAM, and EKS using reusable modules | Not a static YAML-only deployment |
| Centralized GitOps Control Plane â€” ArgoCD runs in control cluster and deploys apps to dev/prod clusters | Not a CI/CD-only showcase without real infrastructure |
| ğŸš€ Production-Grade Deployment â€” NGINX with rolling updates, probes, and health checks | Not a local Minikube experiment |
| ğŸ“ˆ Auto-Scaling Enabled â€” Horizontal Pod Autoscaler (HPA) based on CPU metrics | Not a toy monitoring setup without scaling validation |
| ğŸ“Š Observability Integrated â€” Metrics Server + Prometheus + Grafana | Not a slide-based architecture without live proof |
| ğŸ” Secure by Design â€” IAM roles, OIDC (IRSA), private subnets, controlled networking |  |
| ğŸ§± Modular & Scalable Architecture â€” Designed for real-world extensibility |  |


This project demonstrates a **real, deployable, multi-cluster cloud-native platform** â€” built and validated end-to-end.
## ğŸ§° Tech Stack:

This project combines Infrastructure as Code, Kubernetes orchestration, and GitOps-driven deployment to build a production-style multi-cluster platform.

### â˜ Cloud Platform
- **AWS (ap-south-1)** â€“ Primary cloud provider
- **Amazon EKS** â€“ Managed Kubernetes control plane
- **Amazon VPC** â€“ Custom networking (public/private subnets, NAT, IGW),
- Flow:Internet â†’ Public Subnet â†’ NAT â†’ Private Subnet â†’ EKS Nodes â†’ Pods
  
### ğŸ” Identity & Access (IRSA)
IIAM Roles for Service Accounts (IRSA) allows Kubernetes pods to securely access AWS services.
Instead of storing AWS credentials inside containers:
- Each pod is linked to an IAM role
- AWS verifies identity using OIDC (OpenID Connect)
ğŸ‘‰ Flow:
Pod â†’ OIDC identity â†’ IAM Role â†’ AWS service
- **KMS** â€“ Encryption at rest for cluster secrets
- **CloudWatch** â€“ Control plane logging
- **S3 + DynamoDB** â€“ Terraform remote backend & state locking

### ğŸ— Infrastructure as Code
- **Terraform (>= 1.5)** â€“ Modular infrastructure provisioning
- Reusable modules: `vpc`, `eks`, `iam`
- Remote state management for safe multi-user workflows

### â˜¸ Container Orchestration
- **Kubernetes (EKS 1.29+)**
- **Managed Node Groups**
- **Horizontal Pod Autoscaler (HPA)**
- Rolling updates & self-healing deployments

### ğŸ” GitOps & Deployment

ArgoCD acts as the GitOps controller running inside the control cluster.

Flow:
- ArgoCD watches the GitOps repository
- Detects changes in Kubernetes manifests
- Connects to target clusters (dev / prod) using stored cluster credentials
- Applies changes automatically and keeps clusters in sync with Git

ğŸ‘‰ This ensures Git is always the single source of truth for deployments

### ğŸ“¦ Application Layer
- **Docker** â€“ Containerized Nginx application
- Kubernetes manifests:
  - Deployment
  - Service
  - HPA
  - Namespace

### ğŸ›  Tooling
- kubectl

## ğŸ“¸ Infrastructure Proof

### 1ï¸âƒ£ Multi-Environment EKS Clusters
![EKS Clusters](eks-clusters.png)

---

### 2ï¸âƒ£ Custom VPC Architecture
Public & private subnets across multiple AZs with proper routing.

![VPC Architecture](proof/vpc-architecture.png)

---

### 3ï¸âƒ£ Worker Nodes (Managed Node Groups)
EKS-managed EC2 instances running in private subnets.

![EC2 Nodes](proof/ec2-nodes.png)

---

### 4ï¸âƒ£ GitOps Deployment via ArgoCD
Applications synced and healthy across dev & prod clusters.

![ArgoCD Sync](proof/argocd-sync.png)

---

## ğŸ”„ Request Lifecycle:

***End-to-End Flow:***

**Runtime Request Flow**
1. User sends request to Kubernetes Service
2. Service forwards traffic to one of the running Pods
3. Pod processes the request (Nginx container)
4. Metrics Server collects CPU usage
5. HPA evaluates metrics:
   - If CPU > threshold â†’ increase replicas
   - If CPU normal â†’ maintain or reduce replicas

ğŸ‘‰ This creates a self-healing and auto-scaling system without manual intervention.

***Why This Design?***

- Clear separation of infra and app layers.
- Multi-environment isolation.
- Git-driven declarative deployment.
- Production-aligned Kubernetes architecture.

## ğŸ›¡ Resilience & Security:

***Failure Scenarios***
- Node failure â†’ Pods rescheduled automatically.
- Pod crash â†’ Kubernetes self-healing restarts container.
- High traffic â†’ HPA scales replicas.
- Terraform drift â†’ Reconciliation via `terraform apply`.

***Security Considerations***
- Private subnets for worker nodes.
- IAM least-privilege roles.
- IRSA for workload identity.
- Encrypted EKS secrets via KMS.
- Remote state locking via DynamoDB.

***Scalability & Performance***
- Managed node group scaling.
- Horizontal Pod Autoscaler.
- Multi-AZ subnet distribution.
- Stateless application design.

## âš¡ Quickstart (30-Second Run) :

> Prerequisites:
> - EKS clusters (control, dev, prod) already provisioned via `clusterforge-infra`
> - ArgoCD installed on the control cluster
> - kubectl configured
> - ArgoCD CLI installed

***Switch to Control Cluster***
```bash
kubectl config use-context <control-cluster-context>
```

***2ï¸âƒ£ Apply ArgoCD Applications***

Deploy Dev and Prod applications:
```
kubectl apply -f environments/dev/app.yaml
kubectl apply -f environments/prod/app.yaml
```
***3ï¸âƒ£ Verify ArgoCD Sync***

- argocd app list
- You should see:
```
nginx-dev â†’ Synced & Healthy
nginx-prod â†’ Synced & Healthy
```
***4ï¸âƒ£ Validate Deployment in Target Cluster***

Switch to dev or prod cluster:
```
kubectl config use-context <dev-cluster-context>
kubectl get pods -n nginx-app
kubectl get hpa -n nginx-app
```
You should see:
```
Running NGINX pods
```
HPA configured and active

## âš™ Engineering Philosophy:

***Trade-offs & Decisions***

- **Chose EKS over self-managed Kubernetes**
  â†’ Offloads control plane management, upgrades, and HA complexity to AWS.  
  â†’ Focus stays on platform design and workload reliability instead of etcd and master node operations.
- **Separated infra and GitOps repositories**
  â†’ Enforces clear ownership boundaries between platform and application layers.  
  â†’ Reduces blast radius and aligns with real-world DevOps team structures.
- **Used Managed Node Groups**
  â†’ Simplifies lifecycle management (auto-repair, scaling, upgrades).  
  â†’ Avoids operational overhead of maintaining custom worker AMIs and autoscaling groups.
- **Prioritized Infrastructure as Code over manual console setup**
  â†’ Guarantees reproducibility and auditability.  
  â†’ Eliminates configuration drift and enables safe teardown/rebuild cycles.

***Explicit Limitations***
- No production-grade ingress controller (for simplicity).
- No service mesh implemented.
- Monitoring stack optional (not hardened for production).

### ğŸ™Œ Contributions Welcome!
ClusterForge is an open-source initiative, and we welcome contributions from developers, data scientists, cloud engineers, and Devops enthusiasts!

### ğŸ’¡ Ideas You Can Work On:

- Add production-grade Ingress + ALB.
- Integrate full Prometheus/Grafana monitoring.
- Implement CI validation for Terraform plans.
- Add cost optimization policies.
- Introduce blue/green deployment strategy.

### ğŸ› ï¸ How to Contribute:
- ğŸ´ Fork the repo
- ğŸ“¦ Create a new feature branch: ```git checkout -b feature-name```
- âœ… Make your changes and test them
- ğŸ“¬ Submit a pull request describing your enhancement
- ğŸ¤ Let's Build This Together! Made with ğŸ’š by Manas Gantait

